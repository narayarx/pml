---
title: "pml - project"
author: "Ramesh Narayanan"
date: "Sunday, September 13, 2015"
output: html_document

Data Preparation:
I found that there were many colums that had N/A; since these features were not expected to add anything to the model. I removed those colums before creating the model. Removing those colums left 54 features for training.


Cross Validation and Model Building:
I used random forest with cross-validation for this project. I tried 5 fold and 2 fold CV with one repeat. I found that the accuracy was 98.98% with the 5 fold and 98.56% for the 2 fold CV on the training set. I divided the original training set into 70% training and 30% test for CV.  

I have code that was used to create the model from the training set. I also show screen shots of commands that I ran to get more information about the model like the confusion Matrix. The model generated by using 5 fold CV is called model while that generated by 2 fold CV is called model_2fold. In addition, I also show a table of predicted values vs. actual values for both the models. Based on that table, the calculated accuracy increases from 99.28% to 99.7% when going from 2 fold to 5 fold cross validation.

Given that very accuracies are being attained at this level of CV, it does appear to offer any further advantage to increase the number of folds for the CV or the number of repeats for each cross validation. 

Note - The ''' has been removed below so the R code below will not run.

{r}
library(caret)

data1 = read.csv("c:\\coursera\\practical machine learning\\pml-training-1.csv")

inTrain = createDataPartition(y=data1$classe, p=0.7, list=FALSE)

training = data1[inTrain,]

testing = data1[-inTrain,]

tr = trainControl(method="cv", number=5, repeats=1)

model = train(classe ~., method="rf", trControl=tr, data=training, prox=TRUE)

pred = predict(model, testing)

testing$predRight <- pred==testing$classe

table(pred, testing$classe)
```

>model
Random Forest 

13737 samples
53 predictor
5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 9158, 9159, 9157 
Resampling results across tuning parameters:
  
  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
2    0.9881341  0.9849880  0.002188907  0.002769933
29    0.9898088  0.9871082  0.003026798  0.003828811
57    0.9823104  0.9776202  0.001331425  0.001685039

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 29. 

print(model$finalModel)

Call:
  randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE) 
Type of random forest: classification
Number of trees: 500
No. of variables tried at each split: 29

OOB estimate of  error rate: 0.71%
Confusion matrix:
  A    B    C    D    E class.error
A 3900    5    0    0    1 0.001536098
B   22 2631    5    0    0 0.010158014
C    0    7 2380    9    0 0.006677796
D    0    1   28 2221    2 0.013765542
E    0    1    8    8 2508 0.006732673


table(pred, testing$classe)

pred    A    B    C    D    E
A 1673    9    0    0    0
B    0 1127    7    1    0
C    1    2 1017   15    0
D    0    1    2  948    5
E    0    0    0    0 1077

Accuracy calculated based on the table above = 5842/5885 = 99.7%


 >model_2fold
Random Forest 

13737 samples
   53 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (2 fold) 
Summary of sample sizes: 6869, 6868 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9836209  0.9792761  0.001130755  0.001433910
  29    0.9856594  0.9818577  0.003601747  0.004561061
  57    0.9768514  0.9707119  0.010910235  0.013810320

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 29. 
__________________________________________________________________________

> table(pred_2fold, testing$classe)
          
pred_2fold    A    B    C    D    E
         A 1673    9    0    0    0
         B    0 1127    8    1    0
         C    1    3 1016   13    0
         D    0    0    2  950    5
         E    0    0    0    0 1077
         
         Accuracy calculated based on the table above = 5843/5885 = 99.28%

__________________________________________________________________________

 print(model_2fold)
Random Forest 

13737 samples
   53 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (2 fold) 
Summary of sample sizes: 6869, 6868 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9836209  0.9792761  0.001130755  0.001433910
  29    0.9856594  0.9818577  0.003601747  0.004561061
  57    0.9768514  0.9707119  0.010910235  0.013810320

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 29. 

______________________________________________________________________________
